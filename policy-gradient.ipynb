{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6a7154a-50b1-4df4-aaa9-219e7c013c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abcb4c66-b347-4396-8742-39965bfc4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb26ab1a-b0f6-4610-b5d2-c2bb3f96f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 obs | 2 acts\n"
     ]
    }
   ],
   "source": [
    "# environment parameters\n",
    "env_name=\"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "acts_dim = env.action_space.n\n",
    "print(f\"{obs_dim} obs | {acts_dim} acts\")\n",
    "\n",
    "# mlp parameters\n",
    "hidden_sizes = [10,5]\n",
    "sizes = [obs_dim]+hidden_sizes+[n_acts]\n",
    "\n",
    "# training parameters\n",
    "epochs=50\n",
    "batch_size=5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a457f68-9a70-43fa-b42e-de72bd45af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for j in range(len(sizes)-1):\n",
    "            act = activation if j < len(sizes)-2 else output_activation\n",
    "            layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ddfaa5f9-adbe-48e5-b085-5dcf41440e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Policy Network\n",
    "model = MLP(sizes)\n",
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "def get_action(obs): # only one observation as input\n",
    "    # you can remove .item() to pass batch of obs as input\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "obs = torch.Tensor([1,0,5,1])\n",
    "act = get_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1aa796cd-bd94-4ca5-878b-385499456d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Loss Function\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    logp = get_policy(obs).log_prob(act)\n",
    "    return -(logp * weights).mean()\n",
    "loss = compute_loss(obs, act, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602e8df-d73c-4486-a073-95073716e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs = env.reset()       # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "    while True:\n",
    "        batch_obs.append(obs.copy())\n",
    "        \n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _, _ = env.step(act)\n",
    "        \n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "    \n",
    "    print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "            (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
