{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space.low, env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4    -4.     -0.2099 -4.    ] [2.4    4.     0.2099 4.    ]\n"
     ]
    }
   ],
   "source": [
    "max_obs_values = np.array([2.4, 4, 0.2099 , 4])\n",
    "min_obs_values = np.array([-2.4, -4, -0.2099, -4])\n",
    "print(min_obs_values, max_obs_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETE_OBS_SPACE_SIZE = [10]* len(max_obs_values) # these are the what the max values will corresponds to\n",
    "discrete_obs_space_step_size = (max_obs_values - min_obs_values) / DISCRETE_OBS_SPACE_SIZE\n",
    "\n",
    "def discretizer(obs):\n",
    "    # obs = np.array([obs[0], obs[2]])\n",
    "    discrete_obs = (obs - min_obs_values-0.1)/discrete_obs_space_step_size\n",
    "    return tuple(discrete_obs.astype(np.int16)) # tuple to make indexing easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.99\n",
    "EXPLORATION_RATE = 1\n",
    "EXPLORATION_DECAY_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros(DISCRETE_OBS_SPACE_SIZE + [env.action_space.n])\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(q_table[0][0][0][0])\n",
    "print(q_table[(0,0,0,0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4    -4.     -0.2099 -4.    ] [2.4    4.     0.2099 4.    ]\n",
      "[ 0.02870877 -0.03970408  0.04015733 -0.00687193]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 4, 5, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "# obs = np.array([observation[0], observation[2]])\n",
    "obs = observation\n",
    "print(min_obs_values, max_obs_values)\n",
    "print(obs)\n",
    "discrete_obs = (obs - min_obs_values)/discrete_obs_space_step_size\n",
    "discrete_obs = tuple(discrete_obs.astype(np.int16))\n",
    "discrete_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "non_zero_count = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(10):\n",
    "            for l in range(10):\n",
    "                probility = q_table[(i,j,k,l)]\n",
    "                if np.count_nonzero(probility) != 0:\n",
    "                    # print(probility)\n",
    "                    non_zero_count += 1\n",
    "                total_count += 1\n",
    "print(non_zero_count/total_count) # it doesn't even see most of the observation combinations\n",
    "# 0.1793 max i have seen for 10 discrete value for feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "discrete_obs = (4, 5, 1, 3)\n",
    "print(q_table[discrete_obs])\n",
    "\n",
    "# q_table[discrete_obs + (action,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, exploration rate 0.9500100000002275, avg. score 19.11\n",
      "Episode 1000, exploration rate 0.9500100000002275, avg. score 22.55\n",
      "Episode 2000, exploration rate 0.9500100000002275, avg. score 22.83\n",
      "Episode 3000, exploration rate 0.9500100000002275, avg. score 23.44\n",
      "Episode 4000, exploration rate 0.9500100000002275, avg. score 23.39\n",
      "Episode 5000, exploration rate 0.9500100000002275, avg. score 23.75\n",
      "Episode 6000, exploration rate 0.9500100000002275, avg. score 24.86\n",
      "Episode 7000, exploration rate 0.9500100000002275, avg. score 22.11\n",
      "Episode 8000, exploration rate 0.9500100000002275, avg. score 21.39\n",
      "Episode 9000, exploration rate 0.9500100000002275, avg. score 21.27\n",
      "Episode 10000, exploration rate 0.9500100000002275, avg. score 19.86\n",
      "Episode 11000, exploration rate 0.9500100000002275, avg. score 21.17\n",
      "Episode 12000, exploration rate 0.9500100000002275, avg. score 22.75\n",
      "Episode 13000, exploration rate 0.9500100000002275, avg. score 22.63\n",
      "Episode 14000, exploration rate 0.9500100000002275, avg. score 21.59\n",
      "Episode 15000, exploration rate 0.9500100000002275, avg. score 21.1\n",
      "Episode 16000, exploration rate 0.9500100000002275, avg. score 23.43\n",
      "Episode 17000, exploration rate 0.9500100000002275, avg. score 22.4\n",
      "Episode 18000, exploration rate 0.9500100000002275, avg. score 23.3\n",
      "Episode 19000, exploration rate 0.9500100000002275, avg. score 20.98\n",
      "Episode 20000, exploration rate 0.9500100000002275, avg. score 20.23\n",
      "Episode 21000, exploration rate 0.9500100000002275, avg. score 22.7\n",
      "Episode 22000, exploration rate 0.9500100000002275, avg. score 23.9\n",
      "Episode 23000, exploration rate 0.9500100000002275, avg. score 21.15\n",
      "Episode 24000, exploration rate 0.9500100000002275, avg. score 20.8\n",
      "Episode 25000, exploration rate 0.9500100000002275, avg. score 22.04\n",
      "Episode 26000, exploration rate 0.9500100000002275, avg. score 21.74\n",
      "Episode 27000, exploration rate 0.9500100000002275, avg. score 22.93\n",
      "Episode 28000, exploration rate 0.9500100000002275, avg. score 23.2\n",
      "Episode 29000, exploration rate 0.9500100000002275, avg. score 22.53\n",
      "Episode 30000, exploration rate 0.9500100000002275, avg. score 21.46\n",
      "Episode 31000, exploration rate 0.9500100000002275, avg. score 23.36\n",
      "Episode 32000, exploration rate 0.9500100000002275, avg. score 23.53\n",
      "Episode 33000, exploration rate 0.9500100000002275, avg. score 23.86\n",
      "Episode 34000, exploration rate 0.9500100000002275, avg. score 22.81\n",
      "Episode 35000, exploration rate 0.9500100000002275, avg. score 22.57\n",
      "Episode 36000, exploration rate 0.9500100000002275, avg. score 23.08\n",
      "Episode 37000, exploration rate 0.9500100000002275, avg. score 23.71\n",
      "Episode 38000, exploration rate 0.9500100000002275, avg. score 21.78\n",
      "Episode 39000, exploration rate 0.9500100000002275, avg. score 21.88\n",
      "Episode 40000, exploration rate 0.9500100000002275, avg. score 23.65\n",
      "Episode 41000, exploration rate 0.9500100000002275, avg. score 23.61\n",
      "Episode 42000, exploration rate 0.9500100000002275, avg. score 20.62\n",
      "Episode 43000, exploration rate 0.9500100000002275, avg. score 21.66\n",
      "Episode 44000, exploration rate 0.9500100000002275, avg. score 20.3\n",
      "Episode 45000, exploration rate 0.9500100000002275, avg. score 19.13\n",
      "Episode 46000, exploration rate 0.9500100000002275, avg. score 25.1\n",
      "Episode 47000, exploration rate 0.9500100000002275, avg. score 22.05\n",
      "Episode 48000, exploration rate 0.9500100000002275, avg. score 20.42\n",
      "Episode 49000, exploration rate 0.9500100000002275, avg. score 22.35\n",
      "Episode 50000, exploration rate 0.9500100000002275, avg. score 22.21\n",
      "Episode 51000, exploration rate 0.940010000000273, avg. score 21.03\n",
      "Episode 52000, exploration rate 0.9300100000003185, avg. score 24.2\n",
      "Episode 53000, exploration rate 0.920010000000364, avg. score 25.37\n",
      "Episode 54000, exploration rate 0.9100100000004095, avg. score 26.98\n",
      "Episode 55000, exploration rate 0.9000100000004551, avg. score 25.7\n",
      "Episode 56000, exploration rate 0.8900100000005006, avg. score 27.36\n",
      "Episode 57000, exploration rate 0.8800100000005461, avg. score 29.35\n",
      "Episode 58000, exploration rate 0.8700100000005916, avg. score 24.0\n",
      "Episode 59000, exploration rate 0.8600100000006371, avg. score 24.79\n",
      "Episode 60000, exploration rate 0.8500100000006826, avg. score 26.91\n",
      "Episode 61000, exploration rate 0.8400100000007281, avg. score 28.75\n",
      "Episode 62000, exploration rate 0.8300100000007736, avg. score 28.05\n",
      "Episode 63000, exploration rate 0.8200100000008191, avg. score 27.97\n",
      "Episode 64000, exploration rate 0.8100100000008646, avg. score 31.64\n",
      "Episode 65000, exploration rate 0.8000100000009102, avg. score 34.64\n",
      "Episode 66000, exploration rate 0.7900100000009557, avg. score 30.48\n",
      "Episode 67000, exploration rate 0.7800100000010012, avg. score 38.09\n",
      "Episode 68000, exploration rate 0.7700100000010467, avg. score 37.23\n",
      "Episode 69000, exploration rate 0.7600100000010922, avg. score 37.43\n",
      "Episode 70000, exploration rate 0.7500100000011377, avg. score 36.9\n",
      "Episode 71000, exploration rate 0.7400100000011832, avg. score 44.31\n",
      "Episode 72000, exploration rate 0.7300100000012287, avg. score 43.07\n",
      "Episode 73000, exploration rate 0.7200100000012742, avg. score 44.56\n",
      "Episode 74000, exploration rate 0.7100100000013198, avg. score 43.11\n",
      "Episode 75000, exploration rate 0.7000100000013653, avg. score 45.55\n",
      "Episode 76000, exploration rate 0.6900100000014108, avg. score 47.43\n",
      "Episode 77000, exploration rate 0.6800100000014563, avg. score 45.47\n",
      "Episode 78000, exploration rate 0.6700100000015018, avg. score 48.51\n",
      "Episode 79000, exploration rate 0.6600100000015473, avg. score 52.39\n",
      "Episode 80000, exploration rate 0.6500100000015928, avg. score 46.94\n",
      "Episode 81000, exploration rate 0.6400100000016383, avg. score 54.33\n",
      "Episode 82000, exploration rate 0.6300100000016838, avg. score 54.09\n",
      "Episode 83000, exploration rate 0.6200100000017293, avg. score 56.03\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100000\n",
    "for e in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False; score_per_episode = 0\n",
    "    while not done:\n",
    "        discrete_state = discretizer(state) \n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        if EXPLORATION_RATE > np.random.random():\n",
    "             action = env.action_space.sample() \n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        new_discrete_state = discretizer(new_state)\n",
    "        # print(state, discrete_state, action)\n",
    "        \n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "        else:\n",
    "            reward = -1\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "            \n",
    "        state = new_state\n",
    "        # discrete_state = new_discrete_state\n",
    "        score_per_episode += reward\n",
    "        \n",
    "    # if e > num_episodes/2:\n",
    "    EXPLORATION_RATE = max(EXPLORATION_RATE - EXPLORATION_DECAY_RATE, 0)\n",
    "    scores.append(score_per_episode)\n",
    "    if e % 1000 == 0:\n",
    "        print(f\"Episode {e}, exploration rate {EXPLORATION_RATE}, avg. score {np.mean(scores[-100:])}\")\n",
    "    # if score_per_episode > 2000:\n",
    "    #     break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# discrete state values are not really changing much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "num_windows = len(scores) // window_size\n",
    "smoothed_scores = [np.mean(scores[i*window_size:(i+1)*window_size]) for i in range(num_windows)]\n",
    "plt.plot(smoothed_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
