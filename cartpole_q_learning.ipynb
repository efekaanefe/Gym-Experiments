{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual observation max values: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Actual observation min values: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Used observation max values: [ 4.8 24. ]\n",
      "Used observation min values: [ -4.8 -24. ]\n"
     ]
    }
   ],
   "source": [
    "max_obs_values = env.observation_space.high\n",
    "min_obs_values = env.observation_space.low\n",
    "print(f\"Actual observation max values: {max_obs_values}\")\n",
    "print(f\"Actual observation min values: {min_obs_values}\")\n",
    "# This is kinda a lot, and we dont need infinity values, we can change/ignore them\n",
    "# and we can change radians to degree too\n",
    "\n",
    "# lets change them, not sure about this yet\n",
    "# max_obs_values = (max_obs_values[0], 20, np.rad2deg(max_obs_values[2]), 20)\n",
    "# min_obs_values = (min_obs_values[0], -20, np.rad2deg(min_obs_values[2]), -20)\n",
    "\n",
    "# lets ignore them, simpler observations\n",
    "# only position and angle is used\n",
    "max_obs_values = np.array([max_obs_values[0], np.rad2deg(max_obs_values[2])])\n",
    "min_obs_values = np.array([min_obs_values[0], np.rad2deg(min_obs_values[2])])\n",
    "print(f\"Used observation max values: {max_obs_values}\")\n",
    "print(f\"Used observation min values: {min_obs_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48000002, 2.4       ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DISCRETE_OBS_SPACE_SIZE = [20]* len(max_obs_values) # num_bins\n",
    "DISCRETE_OBS_SPACE_SIZE = [20, 30]\n",
    "discrete_obs_space_step_size = (max_obs_values - min_obs_values) / DISCRETE_OBS_SPACE_SIZE\n",
    "discrete_obs_space_step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.97364283e-07, 2.00000000e+01])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discritizer(obs):\n",
    "    obs = np.array([obs[0], obs[2]])\n",
    "    discrete_obs = (obs - min_obs_values)/discrete_obs_space_step_size\n",
    "    return discrete_obs\n",
    "discritizer([-4.8, 2, 24, 6]) \n",
    "# min value corresponds to zero\n",
    "# max value corresponds to discrete space size for that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETE_SIZE = ()\n",
    "\n",
    "def discretizer(observation):\n",
    "    return [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'discretizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m      3\u001b[0m     observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 4\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretizer\u001b[49m(\u001b[38;5;241m*\u001b[39mobservation) \n\u001b[0;32m      5\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m; score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# action = choose_action(current_state)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# if exploration_rate(e) > np.random.random():\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'discretizer' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# num_episodes = 1\n",
    "# for e in range(num_episodes):\n",
    "#     observation, info = env.reset()\n",
    "#     current_state = discretizer(*observation) \n",
    "#     done = False; score = 0\n",
    "#     while not done:\n",
    "#         # action = choose_action(current_state)\n",
    "#         # if exploration_rate(e) > np.random.random():\n",
    "#         action = env.action_space.sample() \n",
    "\n",
    "#         observation, reward, done, _, _ = env.step(action)\n",
    "#         new_state = discretizer(*observation)\n",
    "\n",
    "#         # update\n",
    "#         # learnt_value = agent.new_Q_value(reward , new_state )\n",
    "#         # old_value = agent.Q_table[current_state][action]\n",
    "#         # agent.Q_table[current_state][action] = (1-agent.lr)*old_value + agent.lr*learnt_value\n",
    "\n",
    "\n",
    "#         current_state = new_state\n",
    "#         score += reward\n",
    "#         env.render()\n",
    "\n",
    "#         # print(current_state, observation)\n",
    "#     print(f\"Episode {e}, score {score}\")\n",
    "\n",
    "# env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
