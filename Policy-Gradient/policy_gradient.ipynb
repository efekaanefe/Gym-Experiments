{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a7154a-50b1-4df4-aaa9-219e7c013c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcb4c66-b347-4396-8742-39965bfc4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb26ab1a-b0f6-4610-b5d2-c2bb3f96f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 obs | 2 acts\n"
     ]
    }
   ],
   "source": [
    "# environment parameters\n",
    "env_name=\"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "acts_dim = env.action_space.n\n",
    "print(f\"{obs_dim} obs | {acts_dim} acts\")\n",
    "\n",
    "# mlp parameters\n",
    "hidden_sizes = [32]\n",
    "sizes = [obs_dim]+hidden_sizes+[acts_dim]\n",
    "\n",
    "# training parameters\n",
    "epochs=100\n",
    "batch_size=5000\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a457f68-9a70-43fa-b42e-de72bd45af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for j in range(len(sizes)-1):\n",
    "            act = activation if j < len(sizes)-2 else output_activation\n",
    "            layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfaa5f9-adbe-48e5-b085-5dcf41440e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "def get_action(obs): # only one observation as input\n",
    "    # you can remove .item() to pass batch of obs as input\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    \"\"\"\n",
    "    Even though we describe this as a loss function, it is not a loss function in the typical sense from supervised learning.\n",
    "    1. The data distribution depends on the parameters\n",
    "    2. It doesnâ€™t measure performance\n",
    "    \"\"\"\n",
    "    logp = get_policy(obs).log_prob(act)\n",
    "    return -(logp * weights).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9470ec2-c83d-4aad-b297-a7aab41c82e1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2602e8df-d73c-4486-a073-95073716e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 22.133 \t return: 22.986 \t ep_len: 22.986\n",
      "epoch:   1 \t loss: 21.624 \t return: 25.020 \t ep_len: 25.020\n",
      "epoch:   2 \t loss: 26.439 \t return: 27.657 \t ep_len: 27.657\n",
      "epoch:   3 \t loss: 26.680 \t return: 30.349 \t ep_len: 30.349\n",
      "epoch:   4 \t loss: 28.714 \t return: 32.103 \t ep_len: 32.103\n",
      "epoch:   5 \t loss: 31.471 \t return: 36.158 \t ep_len: 36.158\n",
      "epoch:   6 \t loss: 34.872 \t return: 39.698 \t ep_len: 39.698\n",
      "epoch:   7 \t loss: 37.969 \t return: 44.434 \t ep_len: 44.434\n",
      "epoch:   8 \t loss: 40.971 \t return: 48.000 \t ep_len: 48.000\n",
      "epoch:   9 \t loss: 43.462 \t return: 53.000 \t ep_len: 53.000\n",
      "epoch:  10 \t loss: 41.648 \t return: 54.290 \t ep_len: 54.290\n",
      "epoch:  11 \t loss: 42.443 \t return: 56.191 \t ep_len: 56.191\n",
      "epoch:  12 \t loss: 56.620 \t return: 66.026 \t ep_len: 66.026\n",
      "epoch:  13 \t loss: 55.302 \t return: 68.685 \t ep_len: 68.685\n",
      "epoch:  14 \t loss: 46.511 \t return: 64.615 \t ep_len: 64.615\n",
      "epoch:  15 \t loss: 53.508 \t return: 70.732 \t ep_len: 70.732\n",
      "epoch:  16 \t loss: 57.788 \t return: 77.000 \t ep_len: 77.000\n",
      "epoch:  17 \t loss: 75.447 \t return: 91.091 \t ep_len: 91.091\n",
      "epoch:  18 \t loss: 69.741 \t return: 89.571 \t ep_len: 89.571\n",
      "epoch:  19 \t loss: 75.647 \t return: 102.612 \t ep_len: 102.612\n",
      "epoch:  20 \t loss: 75.928 \t return: 103.163 \t ep_len: 103.163\n",
      "epoch:  21 \t loss: 82.997 \t return: 112.178 \t ep_len: 112.178\n",
      "epoch:  22 \t loss: 63.110 \t return: 96.731 \t ep_len: 96.731\n",
      "epoch:  23 \t loss: 74.712 \t return: 111.644 \t ep_len: 111.644\n",
      "epoch:  24 \t loss: 84.988 \t return: 122.927 \t ep_len: 122.927\n",
      "epoch:  25 \t loss: 91.313 \t return: 139.056 \t ep_len: 139.056\n",
      "epoch:  26 \t loss: 99.767 \t return: 156.594 \t ep_len: 156.594\n",
      "epoch:  27 \t loss: 100.443 \t return: 153.121 \t ep_len: 153.121\n",
      "epoch:  28 \t loss: 110.839 \t return: 167.300 \t ep_len: 167.300\n",
      "epoch:  29 \t loss: 120.564 \t return: 170.967 \t ep_len: 170.967\n",
      "epoch:  30 \t loss: 125.697 \t return: 186.741 \t ep_len: 186.741\n",
      "epoch:  31 \t loss: 117.600 \t return: 174.000 \t ep_len: 174.000\n",
      "epoch:  32 \t loss: 148.142 \t return: 208.792 \t ep_len: 208.792\n",
      "epoch:  33 \t loss: 141.049 \t return: 183.138 \t ep_len: 183.138\n",
      "epoch:  34 \t loss: 130.248 \t return: 195.923 \t ep_len: 195.923\n",
      "epoch:  35 \t loss: 199.268 \t return: 269.474 \t ep_len: 269.474\n",
      "epoch:  36 \t loss: 199.893 \t return: 318.062 \t ep_len: 318.062\n",
      "epoch:  37 \t loss: 171.643 \t return: 279.611 \t ep_len: 279.611\n",
      "epoch:  38 \t loss: 166.919 \t return: 284.278 \t ep_len: 284.278\n",
      "epoch:  39 \t loss: 139.229 \t return: 240.286 \t ep_len: 240.286\n",
      "epoch:  40 \t loss: 122.816 \t return: 214.625 \t ep_len: 214.625\n",
      "epoch:  41 \t loss: 119.436 \t return: 206.560 \t ep_len: 206.560\n",
      "epoch:  42 \t loss: 129.440 \t return: 222.391 \t ep_len: 222.391\n",
      "epoch:  43 \t loss: 133.815 \t return: 233.045 \t ep_len: 233.045\n",
      "epoch:  44 \t loss: 147.513 \t return: 257.200 \t ep_len: 257.200\n",
      "epoch:  45 \t loss: 151.261 \t return: 253.450 \t ep_len: 253.450\n",
      "epoch:  46 \t loss: 152.808 \t return: 274.579 \t ep_len: 274.579\n",
      "epoch:  47 \t loss: 196.485 \t return: 316.312 \t ep_len: 316.312\n",
      "epoch:  48 \t loss: 202.683 \t return: 357.733 \t ep_len: 357.733\n",
      "epoch:  49 \t loss: 212.988 \t return: 359.143 \t ep_len: 359.143\n",
      "epoch:  50 \t loss: 218.744 \t return: 373.571 \t ep_len: 373.571\n",
      "epoch:  51 \t loss: 270.853 \t return: 428.750 \t ep_len: 428.750\n",
      "epoch:  52 \t loss: 315.096 \t return: 457.727 \t ep_len: 457.727\n",
      "epoch:  53 \t loss: 361.477 \t return: 568.333 \t ep_len: 568.333\n",
      "epoch:  54 \t loss: 290.170 \t return: 504.909 \t ep_len: 504.909\n",
      "epoch:  55 \t loss: 334.867 \t return: 635.625 \t ep_len: 635.625\n",
      "epoch:  56 \t loss: 602.830 \t return: 879.857 \t ep_len: 879.857\n",
      "epoch:  57 \t loss: 354.846 \t return: 636.500 \t ep_len: 636.500\n",
      "epoch:  58 \t loss: 386.537 \t return: 687.250 \t ep_len: 687.250\n",
      "epoch:  59 \t loss: 325.629 \t return: 585.500 \t ep_len: 585.500\n",
      "epoch:  60 \t loss: 303.297 \t return: 518.000 \t ep_len: 518.000\n",
      "epoch:  61 \t loss: 372.401 \t return: 645.500 \t ep_len: 645.500\n",
      "epoch:  62 \t loss: 643.813 \t return: 856.000 \t ep_len: 856.000\n",
      "epoch:  63 \t loss: 338.124 \t return: 604.111 \t ep_len: 604.111\n",
      "epoch:  64 \t loss: 256.265 \t return: 459.333 \t ep_len: 459.333\n",
      "epoch:  65 \t loss: 214.880 \t return: 403.923 \t ep_len: 403.923\n",
      "epoch:  66 \t loss: 166.546 \t return: 332.500 \t ep_len: 332.500\n",
      "epoch:  67 \t loss: 161.569 \t return: 314.125 \t ep_len: 314.125\n",
      "epoch:  68 \t loss: 157.830 \t return: 324.750 \t ep_len: 324.750\n",
      "epoch:  69 \t loss: 159.495 \t return: 310.941 \t ep_len: 310.941\n",
      "epoch:  70 \t loss: 182.480 \t return: 340.133 \t ep_len: 340.133\n",
      "epoch:  71 \t loss: 209.736 \t return: 401.385 \t ep_len: 401.385\n",
      "epoch:  72 \t loss: 203.399 \t return: 413.692 \t ep_len: 413.692\n",
      "epoch:  73 \t loss: 274.152 \t return: 435.167 \t ep_len: 435.167\n",
      "epoch:  74 \t loss: 294.002 \t return: 611.333 \t ep_len: 611.333\n",
      "epoch:  75 \t loss: 325.720 \t return: 668.500 \t ep_len: 668.500\n",
      "epoch:  76 \t loss: 432.661 \t return: 843.667 \t ep_len: 843.667\n",
      "epoch:  77 \t loss: 573.820 \t return: 1074.200 \t ep_len: 1074.200\n",
      "epoch:  78 \t loss: 355.927 \t return: 734.857 \t ep_len: 734.857\n",
      "epoch:  79 \t loss: 670.455 \t return: 1282.200 \t ep_len: 1282.200\n",
      "epoch:  80 \t loss: 1207.156 \t return: 2192.333 \t ep_len: 2192.333\n",
      "epoch:  81 \t loss: 457.668 \t return: 998.000 \t ep_len: 998.000\n",
      "epoch:  82 \t loss: 279.977 \t return: 576.333 \t ep_len: 576.333\n",
      "epoch:  83 \t loss: 229.977 \t return: 480.545 \t ep_len: 480.545\n",
      "epoch:  84 \t loss: 199.386 \t return: 425.083 \t ep_len: 425.083\n",
      "epoch:  85 \t loss: 194.217 \t return: 417.667 \t ep_len: 417.667\n",
      "epoch:  86 \t loss: 194.532 \t return: 417.615 \t ep_len: 417.615\n",
      "epoch:  87 \t loss: 192.012 \t return: 417.167 \t ep_len: 417.167\n",
      "epoch:  88 \t loss: 221.477 \t return: 460.273 \t ep_len: 460.273\n",
      "epoch:  89 \t loss: 218.457 \t return: 470.818 \t ep_len: 470.818\n",
      "epoch:  90 \t loss: 247.963 \t return: 523.700 \t ep_len: 523.700\n",
      "epoch:  91 \t loss: 260.072 \t return: 554.800 \t ep_len: 554.800\n",
      "epoch:  92 \t loss: 332.811 \t return: 690.500 \t ep_len: 690.500\n",
      "epoch:  93 \t loss: 401.340 \t return: 784.286 \t ep_len: 784.286\n",
      "epoch:  94 \t loss: 508.954 \t return: 1063.600 \t ep_len: 1063.600\n",
      "epoch:  95 \t loss: 876.043 \t return: 1706.000 \t ep_len: 1706.000\n",
      "epoch:  96 \t loss: 1175.999 \t return: 2539.500 \t ep_len: 2539.500\n",
      "epoch:  97 \t loss: 1398.399 \t return: 3028.500 \t ep_len: 3028.500\n",
      "epoch:  98 \t loss: 667.913 \t return: 1401.500 \t ep_len: 1401.500\n",
      "epoch:  99 \t loss: 426.326 \t return: 847.333 \t ep_len: 847.333\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "model = MLP(sizes)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs, _ = env.reset()    \n",
    "    done = False            \n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        batch_obs.append(obs.copy())\n",
    "        \n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _, _ = env.step(act)\n",
    "        \n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "        \n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "\n",
    "\n",
    "    # take a single policy gradient update step using the experience gained\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "            (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0f1c5a6-ed0c-44b5-9088-0db43bd6dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1177.0, 634.0, 609.0, 554.0, 1198.0, 912.0], 5084)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_rets, len(batch_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260aa659-2a2b-434c-9674-255b6e76b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"{env_name}_{ep_len}\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c61e2f-94b4-4b5c-bbbc-57fce2beb326",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb77e4a5-2335-4e61-84c9-ea977b1fcc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, score 10.0\n",
      "Episode 1, score 10.0\n",
      "Episode 2, score 10.0\n",
      "Episode 3, score 10.0\n",
      "Episode 4, score 10.0\n",
      "Episode 5, score 10.0\n",
      "Episode 6, score 10.0\n",
      "Episode 7, score 10.0\n",
      "Episode 8, score 10.0\n",
      "Episode 9, score 10.0\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"CartPole-v1_1276\"\n",
    "# model = torch.load(f\"models\\\\{model_name}\")\n",
    "\n",
    "env = gym.make(env_name, render_mode = \"human\")\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False; score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        score += reward\n",
    "        env.render()\n",
    "        if score % 10 == 0:\n",
    "            print(f\"Episode {e}, score {score}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
