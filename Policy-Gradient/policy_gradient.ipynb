{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a7154a-50b1-4df4-aaa9-219e7c013c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcb4c66-b347-4396-8742-39965bfc4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb26ab1a-b0f6-4610-b5d2-c2bb3f96f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 obs | 2 acts\n"
     ]
    }
   ],
   "source": [
    "# environment parameters\n",
    "env_name=\"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "acts_dim = env.action_space.n\n",
    "print(f\"{obs_dim} obs | {acts_dim} acts\")\n",
    "\n",
    "# mlp parameters\n",
    "hidden_sizes = [32]\n",
    "sizes = [obs_dim]+hidden_sizes+[acts_dim]\n",
    "\n",
    "# training parameters\n",
    "epochs=50\n",
    "batch_size=5000\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a457f68-9a70-43fa-b42e-de72bd45af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for j in range(len(sizes)-1):\n",
    "            act = activation if j < len(sizes)-2 else output_activation\n",
    "            layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfaa5f9-adbe-48e5-b085-5dcf41440e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "def get_action(obs): # only one observation as input\n",
    "    # you can remove .item() to pass batch of obs as input\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    logp = get_policy(obs).log_prob(act)\n",
    "    return -(logp * weights).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2602e8df-d73c-4486-a073-95073716e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EFO\\AppData\\Local\\Temp\\ipykernel_21708\\3149500761.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 16.430 \t return: 19.913 \t ep_len: 19.913\n",
      "epoch:   1 \t loss: 19.935 \t return: 22.734 \t ep_len: 22.734\n",
      "epoch:   2 \t loss: 22.266 \t return: 25.646 \t ep_len: 25.646\n",
      "epoch:   3 \t loss: 27.589 \t return: 28.432 \t ep_len: 28.432\n",
      "epoch:   4 \t loss: 31.144 \t return: 32.732 \t ep_len: 32.732\n",
      "epoch:   5 \t loss: 32.936 \t return: 34.628 \t ep_len: 34.628\n",
      "epoch:   6 \t loss: 37.001 \t return: 40.320 \t ep_len: 40.320\n",
      "epoch:   7 \t loss: 44.630 \t return: 47.886 \t ep_len: 47.886\n",
      "epoch:   8 \t loss: 53.061 \t return: 57.575 \t ep_len: 57.575\n",
      "epoch:   9 \t loss: 51.824 \t return: 59.282 \t ep_len: 59.282\n",
      "epoch:  10 \t loss: 60.625 \t return: 65.961 \t ep_len: 65.961\n",
      "epoch:  11 \t loss: 60.103 \t return: 71.014 \t ep_len: 71.014\n",
      "epoch:  12 \t loss: 56.729 \t return: 70.493 \t ep_len: 70.493\n",
      "epoch:  13 \t loss: 65.323 \t return: 78.250 \t ep_len: 78.250\n",
      "epoch:  14 \t loss: 67.315 \t return: 84.867 \t ep_len: 84.867\n",
      "epoch:  15 \t loss: 75.611 \t return: 104.367 \t ep_len: 104.367\n",
      "epoch:  16 \t loss: 91.308 \t return: 119.452 \t ep_len: 119.452\n",
      "epoch:  17 \t loss: 80.565 \t return: 114.341 \t ep_len: 114.341\n",
      "epoch:  18 \t loss: 101.753 \t return: 136.730 \t ep_len: 136.730\n",
      "epoch:  19 \t loss: 116.775 \t return: 163.613 \t ep_len: 163.613\n",
      "epoch:  20 \t loss: 123.643 \t return: 170.097 \t ep_len: 170.097\n",
      "epoch:  21 \t loss: 111.755 \t return: 156.875 \t ep_len: 156.875\n",
      "epoch:  22 \t loss: 131.582 \t return: 181.643 \t ep_len: 181.643\n",
      "epoch:  23 \t loss: 150.218 \t return: 218.913 \t ep_len: 218.913\n",
      "epoch:  24 \t loss: 138.193 \t return: 180.857 \t ep_len: 180.857\n",
      "epoch:  25 \t loss: 150.177 \t return: 231.318 \t ep_len: 231.318\n",
      "epoch:  26 \t loss: 137.113 \t return: 219.375 \t ep_len: 219.375\n",
      "epoch:  27 \t loss: 159.211 \t return: 240.857 \t ep_len: 240.857\n",
      "epoch:  28 \t loss: 125.319 \t return: 201.560 \t ep_len: 201.560\n",
      "epoch:  29 \t loss: 126.073 \t return: 200.231 \t ep_len: 200.231\n",
      "epoch:  30 \t loss: 130.360 \t return: 222.087 \t ep_len: 222.087\n",
      "epoch:  31 \t loss: 139.835 \t return: 236.409 \t ep_len: 236.409\n",
      "epoch:  32 \t loss: 137.736 \t return: 235.182 \t ep_len: 235.182\n",
      "epoch:  33 \t loss: 161.334 \t return: 273.211 \t ep_len: 273.211\n",
      "epoch:  34 \t loss: 221.023 \t return: 347.400 \t ep_len: 347.400\n",
      "epoch:  35 \t loss: 285.192 \t return: 423.333 \t ep_len: 423.333\n",
      "epoch:  36 \t loss: 444.007 \t return: 621.556 \t ep_len: 621.556\n",
      "epoch:  37 \t loss: 369.925 \t return: 493.636 \t ep_len: 493.636\n",
      "epoch:  38 \t loss: 333.484 \t return: 528.300 \t ep_len: 528.300\n",
      "epoch:  39 \t loss: 328.616 \t return: 471.667 \t ep_len: 471.667\n",
      "epoch:  40 \t loss: 361.757 \t return: 477.769 \t ep_len: 477.769\n",
      "epoch:  41 \t loss: 529.307 \t return: 561.727 \t ep_len: 561.727\n",
      "epoch:  42 \t loss: 331.733 \t return: 522.000 \t ep_len: 522.000\n",
      "epoch:  43 \t loss: 391.089 \t return: 538.700 \t ep_len: 538.700\n",
      "epoch:  44 \t loss: 389.478 \t return: 509.900 \t ep_len: 509.900\n",
      "epoch:  45 \t loss: 372.953 \t return: 573.778 \t ep_len: 573.778\n",
      "epoch:  46 \t loss: 590.874 \t return: 903.167 \t ep_len: 903.167\n",
      "epoch:  47 \t loss: 632.952 \t return: 713.667 \t ep_len: 713.667\n",
      "epoch:  48 \t loss: 1275.761 \t return: 1624.000 \t ep_len: 1624.000\n",
      "epoch:  49 \t loss: 790.243 \t return: 749.714 \t ep_len: 749.714\n"
     ]
    }
   ],
   "source": [
    "model = MLP(sizes)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs, _ = env.reset()    # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        batch_obs.append(obs)\n",
    "        \n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _, _ = env.step(act)\n",
    "        \n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "                \n",
    "    # take a single policy gradient update step using the experience gained\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f\"%\n",
    "            (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "260aa659-2a2b-434c-9674-255b6e76b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"{env_name}_{ep_len}\"\n",
    "torch.save(model, PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
