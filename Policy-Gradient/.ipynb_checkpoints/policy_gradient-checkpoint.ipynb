{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a7154a-50b1-4df4-aaa9-219e7c013c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcb4c66-b347-4396-8742-39965bfc4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb26ab1a-b0f6-4610-b5d2-c2bb3f96f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 obs | 2 acts\n"
     ]
    }
   ],
   "source": [
    "# environment parameters\n",
    "env_name=\"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "acts_dim = env.action_space.n\n",
    "print(f\"{obs_dim} obs | {acts_dim} acts\")\n",
    "\n",
    "# mlp parameters\n",
    "hidden_sizes = [32]\n",
    "sizes = [obs_dim]+hidden_sizes+[acts_dim]\n",
    "\n",
    "# training parameters\n",
    "epochs=200\n",
    "batch_size=5000\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a457f68-9a70-43fa-b42e-de72bd45af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for j in range(len(sizes)-1):\n",
    "            act = activation if j < len(sizes)-2 else output_activation\n",
    "            layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfaa5f9-adbe-48e5-b085-5dcf41440e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "def get_action(obs): # only one observation as input\n",
    "    # you can remove .item() to pass batch of obs as input\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    \"\"\"\n",
    "    Even though we describe this as a loss function, it is not a loss function in the typical sense from supervised learning.\n",
    "    1. The data distribution depends on the parameters\n",
    "    2. It doesnâ€™t measure performance\n",
    "    \"\"\"\n",
    "    logp = get_policy(obs).log_prob(act)\n",
    "    return -(logp * weights).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9470ec2-c83d-4aad-b297-a7aab41c82e1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2602e8df-d73c-4486-a073-95073716e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 15.016 \t return: 17.847 \t ep_len: 17.847\n",
      "epoch:   1 \t loss: 16.153 \t return: 19.023 \t ep_len: 19.023\n",
      "epoch:   2 \t loss: 18.478 \t return: 21.778 \t ep_len: 21.778\n",
      "epoch:   3 \t loss: 20.816 \t return: 23.787 \t ep_len: 23.787\n",
      "epoch:   4 \t loss: 23.985 \t return: 27.321 \t ep_len: 27.321\n",
      "epoch:   5 \t loss: 30.422 \t return: 30.951 \t ep_len: 30.951\n",
      "epoch:   6 \t loss: 30.160 \t return: 35.582 \t ep_len: 35.582\n",
      "epoch:   7 \t loss: 31.069 \t return: 37.193 \t ep_len: 37.193\n",
      "epoch:   8 \t loss: 28.830 \t return: 36.577 \t ep_len: 36.577\n",
      "epoch:   9 \t loss: 32.102 \t return: 40.724 \t ep_len: 40.724\n",
      "epoch:  10 \t loss: 34.394 \t return: 43.138 \t ep_len: 43.138\n",
      "epoch:  11 \t loss: 42.489 \t return: 52.896 \t ep_len: 52.896\n",
      "epoch:  12 \t loss: 34.518 \t return: 46.229 \t ep_len: 46.229\n",
      "epoch:  13 \t loss: 35.304 \t return: 48.212 \t ep_len: 48.212\n",
      "epoch:  14 \t loss: 37.580 \t return: 48.462 \t ep_len: 48.462\n",
      "epoch:  15 \t loss: 38.751 \t return: 52.135 \t ep_len: 52.135\n",
      "epoch:  16 \t loss: 39.578 \t return: 54.204 \t ep_len: 54.204\n",
      "epoch:  17 \t loss: 38.511 \t return: 55.209 \t ep_len: 55.209\n",
      "epoch:  18 \t loss: 42.609 \t return: 61.741 \t ep_len: 61.741\n",
      "epoch:  19 \t loss: 42.126 \t return: 60.795 \t ep_len: 60.795\n",
      "epoch:  20 \t loss: 44.983 \t return: 65.416 \t ep_len: 65.416\n",
      "epoch:  21 \t loss: 45.699 \t return: 69.068 \t ep_len: 69.068\n",
      "epoch:  22 \t loss: 42.903 \t return: 65.182 \t ep_len: 65.182\n",
      "epoch:  23 \t loss: 46.284 \t return: 70.208 \t ep_len: 70.208\n",
      "epoch:  24 \t loss: 47.688 \t return: 70.403 \t ep_len: 70.403\n",
      "epoch:  25 \t loss: 54.262 \t return: 77.354 \t ep_len: 77.354\n",
      "epoch:  26 \t loss: 55.103 \t return: 78.828 \t ep_len: 78.828\n",
      "epoch:  27 \t loss: 49.476 \t return: 74.866 \t ep_len: 74.866\n",
      "epoch:  28 \t loss: 50.783 \t return: 81.726 \t ep_len: 81.726\n",
      "epoch:  29 \t loss: 53.221 \t return: 84.949 \t ep_len: 84.949\n",
      "epoch:  30 \t loss: 54.108 \t return: 85.186 \t ep_len: 85.186\n",
      "epoch:  31 \t loss: 54.276 \t return: 86.492 \t ep_len: 86.492\n",
      "epoch:  32 \t loss: 54.864 \t return: 88.018 \t ep_len: 88.018\n",
      "epoch:  33 \t loss: 51.273 \t return: 82.164 \t ep_len: 82.164\n",
      "epoch:  34 \t loss: 55.108 \t return: 86.741 \t ep_len: 86.741\n",
      "epoch:  35 \t loss: 65.218 \t return: 101.220 \t ep_len: 101.220\n",
      "epoch:  36 \t loss: 72.792 \t return: 111.130 \t ep_len: 111.130\n",
      "epoch:  37 \t loss: 82.213 \t return: 130.410 \t ep_len: 130.410\n",
      "epoch:  38 \t loss: 77.724 \t return: 126.175 \t ep_len: 126.175\n",
      "epoch:  39 \t loss: 77.955 \t return: 130.150 \t ep_len: 130.150\n",
      "epoch:  40 \t loss: 106.772 \t return: 162.000 \t ep_len: 162.000\n",
      "epoch:  41 \t loss: 98.670 \t return: 159.500 \t ep_len: 159.500\n",
      "epoch:  42 \t loss: 128.554 \t return: 203.769 \t ep_len: 203.769\n",
      "epoch:  43 \t loss: 128.641 \t return: 220.217 \t ep_len: 220.217\n",
      "epoch:  44 \t loss: 200.065 \t return: 287.500 \t ep_len: 287.500\n",
      "epoch:  45 \t loss: 195.599 \t return: 317.312 \t ep_len: 317.312\n",
      "epoch:  46 \t loss: 160.585 \t return: 269.000 \t ep_len: 269.000\n",
      "epoch:  47 \t loss: 217.302 \t return: 324.188 \t ep_len: 324.188\n",
      "epoch:  48 \t loss: 217.285 \t return: 363.500 \t ep_len: 363.500\n",
      "epoch:  49 \t loss: 240.036 \t return: 387.385 \t ep_len: 387.385\n",
      "epoch:  50 \t loss: 215.726 \t return: 358.643 \t ep_len: 358.643\n",
      "epoch:  51 \t loss: 343.781 \t return: 588.778 \t ep_len: 588.778\n",
      "epoch:  52 \t loss: 297.473 \t return: 492.455 \t ep_len: 492.455\n",
      "epoch:  53 \t loss: 334.527 \t return: 573.000 \t ep_len: 573.000\n",
      "epoch:  54 \t loss: 745.401 \t return: 934.833 \t ep_len: 934.833\n",
      "epoch:  55 \t loss: 569.178 \t return: 839.833 \t ep_len: 839.833\n",
      "epoch:  56 \t loss: 333.676 \t return: 528.900 \t ep_len: 528.900\n",
      "epoch:  57 \t loss: 283.596 \t return: 423.083 \t ep_len: 423.083\n",
      "epoch:  58 \t loss: 226.594 \t return: 359.571 \t ep_len: 359.571\n",
      "epoch:  59 \t loss: 193.982 \t return: 323.000 \t ep_len: 323.000\n",
      "epoch:  60 \t loss: 155.969 \t return: 252.524 \t ep_len: 252.524\n",
      "epoch:  61 \t loss: 145.789 \t return: 229.455 \t ep_len: 229.455\n",
      "epoch:  62 \t loss: 131.062 \t return: 217.565 \t ep_len: 217.565\n",
      "epoch:  63 \t loss: 127.526 \t return: 212.167 \t ep_len: 212.167\n",
      "epoch:  64 \t loss: 129.422 \t return: 212.625 \t ep_len: 212.625\n",
      "epoch:  65 \t loss: 121.445 \t return: 198.423 \t ep_len: 198.423\n",
      "epoch:  66 \t loss: 110.338 \t return: 164.387 \t ep_len: 164.387\n",
      "epoch:  67 \t loss: 121.867 \t return: 198.808 \t ep_len: 198.808\n",
      "epoch:  68 \t loss: 112.996 \t return: 182.536 \t ep_len: 182.536\n",
      "epoch:  69 \t loss: 117.909 \t return: 190.000 \t ep_len: 190.000\n",
      "epoch:  70 \t loss: 116.215 \t return: 173.931 \t ep_len: 173.931\n",
      "epoch:  71 \t loss: 123.334 \t return: 211.167 \t ep_len: 211.167\n",
      "epoch:  72 \t loss: 130.178 \t return: 209.167 \t ep_len: 209.167\n",
      "epoch:  73 \t loss: 132.514 \t return: 227.318 \t ep_len: 227.318\n",
      "epoch:  74 \t loss: 139.752 \t return: 235.273 \t ep_len: 235.273\n",
      "epoch:  75 \t loss: 143.451 \t return: 238.905 \t ep_len: 238.905\n",
      "epoch:  76 \t loss: 151.207 \t return: 265.700 \t ep_len: 265.700\n",
      "epoch:  77 \t loss: 168.953 \t return: 280.526 \t ep_len: 280.526\n",
      "epoch:  78 \t loss: 195.880 \t return: 357.786 \t ep_len: 357.786\n",
      "epoch:  79 \t loss: 283.436 \t return: 502.000 \t ep_len: 502.000\n",
      "epoch:  80 \t loss: 565.304 \t return: 945.167 \t ep_len: 945.167\n",
      "epoch:  81 \t loss: 1309.668 \t return: 1746.667 \t ep_len: 1746.667\n",
      "epoch:  82 \t loss: 1765.160 \t return: 3026.500 \t ep_len: 3026.500\n",
      "epoch:  83 \t loss: 322.964 \t return: 580.667 \t ep_len: 580.667\n",
      "epoch:  84 \t loss: 203.620 \t return: 378.714 \t ep_len: 378.714\n",
      "epoch:  85 \t loss: 152.971 \t return: 286.167 \t ep_len: 286.167\n",
      "epoch:  86 \t loss: 134.544 \t return: 250.200 \t ep_len: 250.200\n",
      "epoch:  87 \t loss: 117.973 \t return: 225.217 \t ep_len: 225.217\n",
      "epoch:  88 \t loss: 102.846 \t return: 199.846 \t ep_len: 199.846\n",
      "epoch:  89 \t loss: 95.627 \t return: 181.250 \t ep_len: 181.250\n",
      "epoch:  90 \t loss: 96.517 \t return: 185.778 \t ep_len: 185.778\n",
      "epoch:  91 \t loss: 87.903 \t return: 172.000 \t ep_len: 172.000\n",
      "epoch:  92 \t loss: 86.294 \t return: 171.500 \t ep_len: 171.500\n",
      "epoch:  93 \t loss: 83.052 \t return: 162.516 \t ep_len: 162.516\n",
      "epoch:  94 \t loss: 83.120 \t return: 162.968 \t ep_len: 162.968\n",
      "epoch:  95 \t loss: 80.595 \t return: 150.265 \t ep_len: 150.265\n",
      "epoch:  96 \t loss: 80.066 \t return: 156.594 \t ep_len: 156.594\n",
      "epoch:  97 \t loss: 78.458 \t return: 153.212 \t ep_len: 153.212\n",
      "epoch:  98 \t loss: 80.165 \t return: 154.576 \t ep_len: 154.576\n",
      "epoch:  99 \t loss: 78.246 \t return: 153.000 \t ep_len: 153.000\n",
      "epoch: 100 \t loss: 77.025 \t return: 145.143 \t ep_len: 145.143\n",
      "epoch: 101 \t loss: 77.612 \t return: 152.939 \t ep_len: 152.939\n",
      "epoch: 102 \t loss: 77.110 \t return: 149.324 \t ep_len: 149.324\n",
      "epoch: 103 \t loss: 77.500 \t return: 148.618 \t ep_len: 148.618\n",
      "epoch: 104 \t loss: 76.643 \t return: 151.000 \t ep_len: 151.000\n",
      "epoch: 105 \t loss: 78.525 \t return: 153.485 \t ep_len: 153.485\n",
      "epoch: 106 \t loss: 80.355 \t return: 151.176 \t ep_len: 151.176\n",
      "epoch: 107 \t loss: 79.833 \t return: 155.970 \t ep_len: 155.970\n",
      "epoch: 108 \t loss: 78.399 \t return: 151.853 \t ep_len: 151.853\n",
      "epoch: 109 \t loss: 79.670 \t return: 154.879 \t ep_len: 154.879\n",
      "epoch: 110 \t loss: 81.353 \t return: 156.438 \t ep_len: 156.438\n",
      "epoch: 111 \t loss: 79.625 \t return: 153.970 \t ep_len: 153.970\n",
      "epoch: 112 \t loss: 82.500 \t return: 159.250 \t ep_len: 159.250\n",
      "epoch: 113 \t loss: 80.911 \t return: 159.125 \t ep_len: 159.125\n",
      "epoch: 114 \t loss: 81.650 \t return: 159.594 \t ep_len: 159.594\n",
      "epoch: 115 \t loss: 80.276 \t return: 154.758 \t ep_len: 154.758\n",
      "epoch: 116 \t loss: 83.507 \t return: 157.094 \t ep_len: 157.094\n",
      "epoch: 117 \t loss: 81.397 \t return: 157.812 \t ep_len: 157.812\n",
      "epoch: 118 \t loss: 85.266 \t return: 160.438 \t ep_len: 160.438\n",
      "epoch: 119 \t loss: 83.977 \t return: 160.781 \t ep_len: 160.781\n",
      "epoch: 120 \t loss: 81.751 \t return: 160.844 \t ep_len: 160.844\n",
      "epoch: 121 \t loss: 82.684 \t return: 162.806 \t ep_len: 162.806\n",
      "epoch: 122 \t loss: 82.428 \t return: 160.562 \t ep_len: 160.562\n",
      "epoch: 123 \t loss: 83.363 \t return: 163.710 \t ep_len: 163.710\n",
      "epoch: 124 \t loss: 83.682 \t return: 164.387 \t ep_len: 164.387\n",
      "epoch: 125 \t loss: 84.288 \t return: 164.452 \t ep_len: 164.452\n",
      "epoch: 126 \t loss: 83.429 \t return: 163.129 \t ep_len: 163.129\n",
      "epoch: 127 \t loss: 86.123 \t return: 166.645 \t ep_len: 166.645\n",
      "epoch: 128 \t loss: 83.356 \t return: 164.548 \t ep_len: 164.548\n",
      "epoch: 129 \t loss: 81.219 \t return: 161.806 \t ep_len: 161.806\n",
      "epoch: 130 \t loss: 84.472 \t return: 164.710 \t ep_len: 164.710\n",
      "epoch: 131 \t loss: 82.596 \t return: 161.871 \t ep_len: 161.871\n",
      "epoch: 132 \t loss: 81.322 \t return: 159.594 \t ep_len: 159.594\n",
      "epoch: 133 \t loss: 82.249 \t return: 159.781 \t ep_len: 159.781\n",
      "epoch: 134 \t loss: 81.242 \t return: 159.781 \t ep_len: 159.781\n",
      "epoch: 135 \t loss: 79.338 \t return: 156.515 \t ep_len: 156.515\n",
      "epoch: 136 \t loss: 80.143 \t return: 159.688 \t ep_len: 159.688\n",
      "epoch: 137 \t loss: 81.704 \t return: 161.484 \t ep_len: 161.484\n",
      "epoch: 138 \t loss: 82.926 \t return: 164.710 \t ep_len: 164.710\n",
      "epoch: 139 \t loss: 82.255 \t return: 163.290 \t ep_len: 163.290\n",
      "epoch: 140 \t loss: 83.288 \t return: 163.419 \t ep_len: 163.419\n",
      "epoch: 141 \t loss: 83.618 \t return: 164.419 \t ep_len: 164.419\n",
      "epoch: 142 \t loss: 82.056 \t return: 163.968 \t ep_len: 163.968\n",
      "epoch: 143 \t loss: 83.461 \t return: 167.833 \t ep_len: 167.833\n",
      "epoch: 144 \t loss: 84.881 \t return: 169.633 \t ep_len: 169.633\n",
      "epoch: 145 \t loss: 87.647 \t return: 174.276 \t ep_len: 174.276\n",
      "epoch: 146 \t loss: 88.811 \t return: 176.759 \t ep_len: 176.759\n",
      "epoch: 147 \t loss: 91.530 \t return: 180.107 \t ep_len: 180.107\n",
      "epoch: 148 \t loss: 91.965 \t return: 181.571 \t ep_len: 181.571\n",
      "epoch: 149 \t loss: 95.558 \t return: 189.667 \t ep_len: 189.667\n",
      "epoch: 150 \t loss: 99.724 \t return: 194.192 \t ep_len: 194.192\n",
      "epoch: 151 \t loss: 102.068 \t return: 200.077 \t ep_len: 200.077\n",
      "epoch: 152 \t loss: 102.716 \t return: 203.160 \t ep_len: 203.160\n",
      "epoch: 153 \t loss: 111.472 \t return: 216.167 \t ep_len: 216.167\n",
      "epoch: 154 \t loss: 115.018 \t return: 223.913 \t ep_len: 223.913\n",
      "epoch: 155 \t loss: 116.786 \t return: 225.913 \t ep_len: 225.913\n",
      "epoch: 156 \t loss: 123.876 \t return: 241.619 \t ep_len: 241.619\n",
      "epoch: 157 \t loss: 128.657 \t return: 248.810 \t ep_len: 248.810\n",
      "epoch: 158 \t loss: 137.625 \t return: 266.737 \t ep_len: 266.737\n",
      "epoch: 159 \t loss: 146.501 \t return: 281.000 \t ep_len: 281.000\n",
      "epoch: 160 \t loss: 150.576 \t return: 289.111 \t ep_len: 289.111\n",
      "epoch: 161 \t loss: 160.208 \t return: 304.941 \t ep_len: 304.941\n",
      "epoch: 162 \t loss: 167.240 \t return: 322.938 \t ep_len: 322.938\n",
      "epoch: 163 \t loss: 181.186 \t return: 339.067 \t ep_len: 339.067\n",
      "epoch: 164 \t loss: 181.582 \t return: 342.067 \t ep_len: 342.067\n",
      "epoch: 165 \t loss: 198.466 \t return: 373.357 \t ep_len: 373.357\n",
      "epoch: 166 \t loss: 207.277 \t return: 386.308 \t ep_len: 386.308\n",
      "epoch: 167 \t loss: 219.523 \t return: 405.846 \t ep_len: 405.846\n",
      "epoch: 168 \t loss: 238.391 \t return: 438.500 \t ep_len: 438.500\n",
      "epoch: 169 \t loss: 238.696 \t return: 440.083 \t ep_len: 440.083\n",
      "epoch: 170 \t loss: 255.449 \t return: 466.273 \t ep_len: 466.273\n",
      "epoch: 171 \t loss: 255.050 \t return: 465.273 \t ep_len: 465.273\n",
      "epoch: 172 \t loss: 261.038 \t return: 469.000 \t ep_len: 469.000\n",
      "epoch: 173 \t loss: 260.477 \t return: 470.545 \t ep_len: 470.545\n",
      "epoch: 174 \t loss: 260.649 \t return: 455.500 \t ep_len: 455.500\n",
      "epoch: 175 \t loss: 256.227 \t return: 430.083 \t ep_len: 430.083\n",
      "epoch: 176 \t loss: 272.137 \t return: 481.273 \t ep_len: 481.273\n",
      "epoch: 177 \t loss: 268.496 \t return: 476.364 \t ep_len: 476.364\n",
      "epoch: 178 \t loss: 252.739 \t return: 401.154 \t ep_len: 401.154\n",
      "epoch: 179 \t loss: 278.936 \t return: 493.364 \t ep_len: 493.364\n",
      "epoch: 180 \t loss: 263.638 \t return: 365.286 \t ep_len: 365.286\n",
      "epoch: 181 \t loss: 261.764 \t return: 427.833 \t ep_len: 427.833\n",
      "epoch: 182 \t loss: 297.949 \t return: 485.545 \t ep_len: 485.545\n",
      "epoch: 183 \t loss: 305.129 \t return: 507.182 \t ep_len: 507.182\n",
      "epoch: 184 \t loss: 408.329 \t return: 698.125 \t ep_len: 698.125\n",
      "epoch: 185 \t loss: 427.950 \t return: 620.222 \t ep_len: 620.222\n",
      "epoch: 186 \t loss: 496.373 \t return: 584.333 \t ep_len: 584.333\n",
      "epoch: 187 \t loss: 599.321 \t return: 766.125 \t ep_len: 766.125\n",
      "epoch: 188 \t loss: 686.536 \t return: 852.143 \t ep_len: 852.143\n",
      "epoch: 189 \t loss: 900.734 \t return: 1297.000 \t ep_len: 1297.000\n",
      "epoch: 190 \t loss: 863.742 \t return: 844.375 \t ep_len: 844.375\n",
      "epoch: 191 \t loss: 1276.161 \t return: 2251.333 \t ep_len: 2251.333\n",
      "epoch: 192 \t loss: 2243.557 \t return: 3974.500 \t ep_len: 3974.500\n",
      "epoch: 193 \t loss: 1006.528 \t return: 1454.000 \t ep_len: 1454.000\n",
      "epoch: 194 \t loss: 1492.063 \t return: 2629.000 \t ep_len: 2629.000\n",
      "epoch: 195 \t loss: 1076.402 \t return: 1815.000 \t ep_len: 1815.000\n",
      "epoch: 196 \t loss: 766.427 \t return: 1070.200 \t ep_len: 1070.200\n",
      "epoch: 197 \t loss: 1017.896 \t return: 1800.333 \t ep_len: 1800.333\n",
      "epoch: 198 \t loss: 6883.144 \t return: 7862.500 \t ep_len: 7862.500\n",
      "epoch: 199 \t loss: 673.286 \t return: 1032.667 \t ep_len: 1032.667\n"
     ]
    }
   ],
   "source": [
    "model = MLP(sizes)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs, _ = env.reset()    # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        batch_obs.append(obs)\n",
    "        \n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _, _ = env.step(act)\n",
    "        \n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "                \n",
    "    # take a single policy gradient update step using the experience gained\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f\"%\n",
    "            (i, np.mean(batch_rets)))\n",
    "\n",
    "# why is loss increasing???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260aa659-2a2b-434c-9674-255b6e76b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"{env_name}_{ep_len}\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c61e2f-94b4-4b5c-bbbc-57fce2beb326",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb77e4a5-2335-4e61-84c9-ea977b1fcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"CartPole-v1_1276\"\n",
    "# model = torch.load(f\"models\\\\{model_name}\")\n",
    "\n",
    "env = gym.make(env_name, render_mode = \"human\")\n",
    "num_episodes = 10\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False; score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        state = new_state\n",
    "        score += reward\n",
    "        env.render()\n",
    "        if score % 100 == 0:\n",
    "            print(f\"Episode {e}, score {score}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
