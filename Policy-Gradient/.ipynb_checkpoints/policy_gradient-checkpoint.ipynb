{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a7154a-50b1-4df4-aaa9-219e7c013c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abcb4c66-b347-4396-8742-39965bfc4c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb26ab1a-b0f6-4610-b5d2-c2bb3f96f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 obs | 2 acts\n"
     ]
    }
   ],
   "source": [
    "# environment parameters\n",
    "env_name=\"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "acts_dim = env.action_space.n\n",
    "print(f\"{obs_dim} obs | {acts_dim} acts\")\n",
    "\n",
    "# mlp parameters\n",
    "hidden_sizes = [32]\n",
    "sizes = [obs_dim]+hidden_sizes+[acts_dim]\n",
    "\n",
    "# training parameters\n",
    "epochs=200\n",
    "batch_size=5000\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a457f68-9a70-43fa-b42e-de72bd45af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for j in range(len(sizes)-1):\n",
    "            act = activation if j < len(sizes)-2 else output_activation\n",
    "            layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddfaa5f9-adbe-48e5-b085-5dcf41440e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_policy(obs):\n",
    "    logits = model(obs)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "def get_action(obs): # only one observation as input\n",
    "    # you can remove .item() to pass batch of obs as input\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    \"\"\"\n",
    "    Even though we describe this as a loss function, it is not a loss function in the typical sense from supervised learning.\n",
    "    1. The data distribution depends on the parameters\n",
    "    2. It doesnâ€™t measure performance\n",
    "    \"\"\"\n",
    "    logp = get_policy(obs).log_prob(act)\n",
    "    return -(logp * weights).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9470ec2-c83d-4aad-b297-a7aab41c82e1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2602e8df-d73c-4486-a073-95073716e621",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m%3d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m return: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m ep_len: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m\n\u001b[0;32m     56\u001b[0m         (i, np\u001b[38;5;241m.\u001b[39mmean(batch_rets)))\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "model = MLP(sizes)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs, _ = env.reset()    # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    # for batch in range(batch_size):\n",
    "    while True:\n",
    "        batch_obs.append(obs.copy())\n",
    "        \n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _, _ = env.step(act)\n",
    "        \n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "\n",
    "    # take a single policy gradient update step using the experience gained\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch: %3d \\t reward: %.3f \"%\n",
    "            (i, np.mean(batch_rets)))\n",
    "\n",
    "# why is loss increasing???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260aa659-2a2b-434c-9674-255b6e76b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"{env_name}_{ep_len}\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c61e2f-94b4-4b5c-bbbc-57fce2beb326",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77e4a5-2335-4e61-84c9-ea977b1fcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"CartPole-v1_1276\"\n",
    "# model = torch.load(f\"models\\\\{model_name}\")\n",
    "\n",
    "env = gym.make(env_name, render_mode = \"human\")\n",
    "num_episodes = 10\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False; score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        state = new_state\n",
    "        score += reward\n",
    "        env.render()\n",
    "        if score % 100 == 0:\n",
    "            print(f\"Episode {e}, score {score}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
