{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space.low, env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4    -5.     -0.2099 -4.    ] [2.4    5.     0.2099 4.    ]\n"
     ]
    }
   ],
   "source": [
    "max_obs_values = np.array([2.4, 5, 0.2099 , 4])\n",
    "min_obs_values = np.array([-2.4, -5, -0.2099, -4])\n",
    "print(min_obs_values, max_obs_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETE_OBS_SPACE_SIZE = [10]* len(max_obs_values) # these are the what the max values will corresponds to\n",
    "discrete_obs_space_step_size = (max_obs_values - min_obs_values) / DISCRETE_OBS_SPACE_SIZE\n",
    "\n",
    "def discretizer(obs):\n",
    "    # obs = np.array([obs[0], obs[2]])\n",
    "    discrete_obs = (obs - min_obs_values)/discrete_obs_space_step_size\n",
    "    return tuple(discrete_obs.astype(np.int16)) # tuple to make indexing easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EXPLORATION_RATE = 1\n",
    "EXPLORATION_DECAY_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros(DISCRETE_OBS_SPACE_SIZE + [env.action_space.n])\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(q_table[0][0][0][0])\n",
    "print(q_table[(0,0,0,0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4    -5.     -0.2099 -4.    ] [2.4    5.     0.2099 4.    ]\n",
      "[-0.0440576   0.02953474  0.02037831  0.04577639]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 5, 5, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "# obs = np.array([observation[0], observation[2]])\n",
    "obs = observation\n",
    "print(min_obs_values, max_obs_values)\n",
    "print(obs)\n",
    "discrete_obs = (obs - min_obs_values)/discrete_obs_space_step_size\n",
    "discrete_obs = tuple(discrete_obs.astype(np.int16))\n",
    "discrete_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1\n",
    "q_table[discrete_obs]\n",
    "# q_table[discrete_obs + (action,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, exploration rate 0.999999, average score 14.0\n",
      "Episode 1000, exploration rate 0.9989989999999712, average score 40.0\n",
      "Episode 2000, exploration rate 0.9979989999999425, average score 12.0\n",
      "Episode 3000, exploration rate 0.9969989999999137, average score 12.0\n",
      "Episode 4000, exploration rate 0.995998999999885, average score 53.0\n",
      "Episode 5000, exploration rate 0.9949989999998562, average score 25.0\n",
      "Episode 6000, exploration rate 0.9939989999998274, average score 26.0\n",
      "Episode 7000, exploration rate 0.9929989999997987, average score 13.0\n",
      "Episode 8000, exploration rate 0.9919989999997699, average score 13.0\n",
      "Episode 9000, exploration rate 0.9909989999997412, average score 9.0\n",
      "Episode 10000, exploration rate 0.9899989999997124, average score 20.0\n",
      "Episode 11000, exploration rate 0.9889989999996837, average score 13.0\n",
      "Episode 12000, exploration rate 0.9879989999996549, average score 36.0\n",
      "Episode 13000, exploration rate 0.9869989999996261, average score 24.0\n",
      "Episode 14000, exploration rate 0.9859989999995974, average score 11.0\n",
      "Episode 15000, exploration rate 0.9849989999995686, average score 13.0\n",
      "Episode 16000, exploration rate 0.9839989999995399, average score 62.0\n",
      "Episode 17000, exploration rate 0.9829989999995111, average score 15.0\n",
      "Episode 18000, exploration rate 0.9819989999994824, average score 12.0\n",
      "Episode 19000, exploration rate 0.9809989999994536, average score 31.0\n",
      "Episode 20000, exploration rate 0.9799989999994249, average score 48.0\n",
      "Episode 21000, exploration rate 0.9789989999993961, average score 12.0\n",
      "Episode 22000, exploration rate 0.9779989999993673, average score 13.0\n",
      "Episode 23000, exploration rate 0.9769989999993386, average score 15.0\n",
      "Episode 24000, exploration rate 0.9759989999993098, average score 38.0\n",
      "Episode 25000, exploration rate 0.9749989999992811, average score 19.0\n",
      "Episode 26000, exploration rate 0.9739989999992523, average score 23.0\n",
      "Episode 27000, exploration rate 0.9729989999992236, average score 25.0\n",
      "Episode 28000, exploration rate 0.9719989999991948, average score 26.0\n",
      "Episode 29000, exploration rate 0.9709989999991661, average score 22.0\n",
      "Episode 30000, exploration rate 0.9699989999991373, average score 29.0\n",
      "Episode 31000, exploration rate 0.9689989999991085, average score 77.0\n",
      "Episode 32000, exploration rate 0.9679989999990798, average score 48.0\n",
      "Episode 33000, exploration rate 0.966998999999051, average score 14.0\n",
      "Episode 34000, exploration rate 0.9659989999990223, average score 17.0\n",
      "Episode 35000, exploration rate 0.9649989999989935, average score 52.0\n",
      "Episode 36000, exploration rate 0.9639989999989648, average score 15.0\n",
      "Episode 37000, exploration rate 0.962998999998936, average score 35.0\n",
      "Episode 38000, exploration rate 0.9619989999989073, average score 32.0\n",
      "Episode 39000, exploration rate 0.9609989999988785, average score 38.0\n",
      "Episode 40000, exploration rate 0.9599989999988497, average score 15.0\n",
      "Episode 41000, exploration rate 0.958998999998821, average score 14.0\n",
      "Episode 42000, exploration rate 0.9579989999987922, average score 35.0\n",
      "Episode 43000, exploration rate 0.9569989999987635, average score 17.0\n",
      "Episode 44000, exploration rate 0.9559989999987347, average score 33.0\n",
      "Episode 45000, exploration rate 0.954998999998706, average score 19.0\n",
      "Episode 46000, exploration rate 0.9539989999986772, average score 27.0\n",
      "Episode 47000, exploration rate 0.9529989999986485, average score 21.0\n",
      "Episode 48000, exploration rate 0.9519989999986197, average score 21.0\n",
      "Episode 49000, exploration rate 0.9509989999985909, average score 11.0\n",
      "Episode 50000, exploration rate 0.9499989999985622, average score 17.0\n",
      "Episode 51000, exploration rate 0.9489989999985334, average score 23.0\n",
      "Episode 52000, exploration rate 0.9479989999985047, average score 33.0\n",
      "Episode 53000, exploration rate 0.9469989999984759, average score 36.0\n",
      "Episode 54000, exploration rate 0.9459989999984472, average score 10.0\n",
      "Episode 55000, exploration rate 0.9449989999984184, average score 25.0\n",
      "Episode 56000, exploration rate 0.9439989999983897, average score 10.0\n",
      "Episode 57000, exploration rate 0.9429989999983609, average score 14.0\n",
      "Episode 58000, exploration rate 0.9419989999983321, average score 10.0\n",
      "Episode 59000, exploration rate 0.9409989999983034, average score 19.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100000\n",
    "average_scores = []\n",
    "for e in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    current_obs = discretizer(observation) \n",
    "    done = False; score_per_episode = 0\n",
    "    scores = [] # list for 100 episodes\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[current_obs])\n",
    "        if EXPLORATION_RATE > np.random.random():\n",
    "             action = env.action_space.sample() \n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        discrete_obs = discretizer(observation)\n",
    "        # print(discrete_obs, done)\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[discrete_obs])\n",
    "            current_q = q_table[current_obs + (action,)]\n",
    "            new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[current_obs + (action,)] = new_q\n",
    "            # print(new_q, current_q) # new_q isn't changing\n",
    "        current_obs = discrete_obs\n",
    "        score_per_episode += reward\n",
    "    EXPLORATION_RATE = max(EXPLORATION_RATE - EXPLORATION_DECAY_RATE, 0)\n",
    "    scores.append(score_per_episode)\n",
    "        \n",
    "    if e % 1000 == 0:\n",
    "        average_score = np.mean(scores)\n",
    "        average_scores.append(average_score)\n",
    "        scores = []\n",
    "        print(f\"Episode {e}, exploration rate {EXPLORATION_RATE}, average score {average_score}\")\n",
    "    if score_per_episode > 10000:\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "# cart position is not really changing much for random actions, maybe i should use all the observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
